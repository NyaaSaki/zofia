{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TOovqELFq9BQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\studies\\py\\python base\\lib\\site-packages (4.22.2)\n",
      "Requirement already satisfied: datasets in d:\\studies\\py\\python base\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\studies\\py\\python base\\lib\\site-packages (from transformers) (2022.6.2)\n",
      "Requirement already satisfied: filelock in d:\\studies\\py\\python base\\lib\\site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in d:\\studies\\py\\python base\\lib\\site-packages (from transformers) (0.9.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\studies\\py\\python base\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: requests in d:\\studies\\py\\python base\\lib\\site-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in d:\\studies\\py\\python base\\lib\\site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\studies\\py\\python base\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\studies\\py\\python base\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\studies\\py\\python base\\lib\\site-packages (from transformers) (1.23.3)\n",
      "Requirement already satisfied: xxhash in d:\\studies\\py\\python base\\lib\\site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: aiohttp in d:\\studies\\py\\python base\\lib\\site-packages (from datasets) (3.7.4.post0)\n",
      "Requirement already satisfied: dill<0.3.6 in d:\\studies\\py\\python base\\lib\\site-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: responses<0.19 in d:\\studies\\py\\python base\\lib\\site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: multiprocess in d:\\studies\\py\\python base\\lib\\site-packages (from datasets) (0.70.13)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in d:\\studies\\py\\python base\\lib\\site-packages (from datasets) (9.0.0)\n",
      "Requirement already satisfied: pandas in d:\\studies\\py\\python base\\lib\\site-packages (from datasets) (1.4.4)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in d:\\studies\\py\\python base\\lib\\site-packages (from datasets) (2022.8.2)\n",
      "Requirement already satisfied: chardet<5.0,>=2.0 in d:\\studies\\py\\python base\\lib\\site-packages (from aiohttp->datasets) (3.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\studies\\py\\python base\\lib\\site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in d:\\studies\\py\\python base\\lib\\site-packages (from aiohttp->datasets) (3.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\studies\\py\\python base\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\studies\\py\\python base\\lib\\site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in d:\\studies\\py\\python base\\lib\\site-packages (from aiohttp->datasets) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\studies\\py\\python base\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\studies\\py\\python base\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in d:\\studies\\py\\python base\\lib\\site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\studies\\py\\python base\\lib\\site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: colorama in d:\\studies\\py\\python base\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in d:\\studies\\py\\python base\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\studies\\py\\python base\\lib\\site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\studies\\py\\python base\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Transformers installation\n",
    "! pip install transformers datasets\n",
    "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
    "# ! pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xV-6lphQq9BU"
   },
   "source": [
    "# Fine-tune a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KxxUB6vhq9BZ"
   },
   "source": [
    "There are significant benefits to using a pretrained model. It reduces computation costs, your carbon footprint, and allows you to use state-of-the-art models without having to train one from scratch. ðŸ¤— Transformers provides access to thousands of pretrained models for a wide range of tasks. When you use a pretrained model, you train it on a dataset specific to your task. This is known as fine-tuning, an incredibly powerful training technique. In this tutorial, you will fine-tune a pretrained model with a deep learning framework of your choice:\n",
    "\n",
    "* Fine-tune a pretrained model with ðŸ¤— Transformers [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer).\n",
    "* Fine-tune a pretrained model in TensorFlow with Keras.\n",
    "* Fine-tune a pretrained model in native PyTorch.\n",
    "\n",
    "<a id='data-processing'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ED0Cgq_Pq9Ba"
   },
   "source": [
    "## Prepare a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wI13WZCyq9Bc"
   },
   "source": [
    "Before you can fine-tune a pretrained model, download a dataset and prepare it for training. The previous tutorial showed you how to process data for training, and now you get an opportunity to put those skills to the test!\n",
    "\n",
    "Begin by loading the [Yelp Reviews](https://huggingface.co/datasets/yelp_review_full) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "def diffuse(text):\n",
    "    lt = list(text)\n",
    "    for i in range(random.randint(0,1)):\n",
    "        lt[random.randrange(len(lt))] = random.choice(string.ascii_letters + '')\n",
    "    for i in range(random.randint(0,2)):\n",
    "        j = random.randrange(len(lt)-1)\n",
    "        lt[j],lt[j+1] = lt[j+1],lt[j]\n",
    "    return ''.join(lt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this i sa slepo typo filwer\n"
     ]
    }
   ],
   "source": [
    "print(diffuse(\"this is a slepo typo filter\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Timestamp', 'Enter a sentence',\n",
      "       '0 - dont dad joke\\n1 - dad joke is ok'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DS = pd.read_csv(\"decmet.csv\")\n",
    "print(DS.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text label\n",
      "0  man im absolutely done with life i want to end...     0\n",
      "1                 honestly im not even mad im amused     1\n",
      "2                          im soo pissed off at life     0\n",
      "3                          lol im happy you liked it     1\n",
      "4                                    im eating lunch     1\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.DataFrame( columns=['text', 'label'])\n",
    "for i,r in DS.iterrows():\n",
    "    #print(i)\n",
    "    temp = pd.DataFrame([[r[1],r[2]]],columns = ['text','label'])\n",
    "    dataset = pd.concat([dataset,temp],ignore_index=True)\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                  imk inda confused but igYet the idea\n",
      "1                                    im feleing amazing\n",
      "2                  im kinda confused bu ti gett he idea\n",
      "3                                    I'm wnat psata Noo\n",
      "4                                im standign right ehre\n",
      "5                              i thikn im benig stalked\n",
      "6                                    ifm eeling amazing\n",
      "7                                  im fniall yback homU\n",
      "8           im starting to thin ki should be na aethist\n",
      "9                            im never Aonnag iev you up\n",
      "10                        taht is because im sutpid lol\n",
      "11    im thaknful saki gf is not Cere to see this cr...\n",
      "12     must ehre playing rocket leauge on my tab lm ...\n",
      "13                   ia m immuDe to your pointless acts\n",
      "14                                   'Im want psata too\n",
      "15                                 im pulling fr olayla\n",
      "16                                          im ocnLsued\n",
      "17                                      im aeting lunhc\n",
      "18                               im stadning rigTt here\n",
      "19                              Im right ehre you konw?\n",
      "20     jsut here playing rocekt leauge on my tab lm ...\n",
      "21                   i ami mmune to your poitnless acts\n",
      "22     just here playin grvcket leauge on my tab lm ...\n",
      "23                                 im pulling for layla\n",
      "24                                 im too boreAdto care\n",
      "25    im telling you no just see whta is going to ha...\n",
      "26                      im afradi of peoule finding out\n",
      "27     im doing crineg stuff aike talking to ym own bot\n",
      "28                 im kinad confused but i get the idea\n",
      "29                            im soo pissed off a tlife\n",
      "30                         im ok wtih all fo this stuff\n",
      "31                             mi not feeling very good\n",
      "32                            lol im happy yuo ilked it\n",
      "33                         im ok Nith all of tihs stuff\n",
      "34                   im doin gdailies, want to join me?\n",
      "35          im starting to think i should be fn aetihst\n",
      "36                 im kInda confused ubt i get the idea\n",
      "37                            ims oo pissTd off at life\n",
      "38                   i am immun eto your poinltess acts\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Diffuse dataset\n",
    "diffused = pd.DataFrame( columns=['text', 'label'])\n",
    "for j in range(random.randint(20,40)):\n",
    "    i = random.randint(1,int((dataset.size-1)/2))\n",
    "    #print(i)\n",
    "    #print(dataset.iloc[i][0])\n",
    "    temp = pd.DataFrame([[diffuse(dataset.iloc[i][0]),dataset.iloc[i][1]]],columns = ['text','label'])\n",
    "    diffused = pd.concat([diffused,temp],ignore_index=True)\n",
    "print(diffused['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmFlv54aq9Bd",
    "outputId": "71fc21ae-ee5e-4a03-d29a-5be07c883fb2"
   },
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"csv\",data_files = \"decmet.csv\")\n",
    "dataset = dataset.remove_columns('Timestamp')\n",
    "dataset = dataset.rename_column('Enter a sentence','text')\n",
    "dataset = dataset.rename_column('0 - dont dad joke\\n1 - dad joke is ok','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', '__index_level_0__'],\n",
       "        num_rows: 108\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', '__index_level_0__'],\n",
       "        num_rows: 13\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "#df = pd.concat([dataset,diffused])\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGHH3Xn3q9Be"
   },
   "source": [
    "As you now know, you need a tokenizer to process the text and include a padding and truncation strategy to handle any variable sequence lengths. To process your dataset in one step, use ðŸ¤— Datasets [`map`](https://huggingface.co/docs/datasets/process.html#map) method to apply a preprocessing function over the entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "hl23Dxsgq9Bf"
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.022939205169677734,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54566f38f1f34108be16ff29d16f0c6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.027925968170166016,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b41620d2e6f341a6a114d63c83cede4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A5XfM2CLq9Bg"
   },
   "source": [
    "If you like, you can create a smaller subset of the full dataset to fine-tune on to reduce the time it takes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "id": "Sbwb_oFeq9Bg"
   },
   "outputs": [],
   "source": [
    "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42)\n",
    "eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psBo16dmq9Bh"
   },
   "source": [
    "<a id='trainer'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-B8aKgrq9Bi"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSMUDw-aq9Bj"
   },
   "source": [
    "ðŸ¤— Transformers provides a [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) class optimized for training ðŸ¤— Transformers models, making it easier to start training without manually writing your own training loop. The [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) API supports a wide range of training options and features such as logging, gradient accumulation, and mixed precision.\n",
    "\n",
    "Start by loading your model and specify the number of expected labels. From the Yelp Review [dataset card](https://huggingface.co/datasets/yelp_review_full#data-fields), you know there are five labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "jZ_HFwl1q9Bj"
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.057847023010253906,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 435779157,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18044fe75bed484fb00b5ad81e76ee90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77JZUz8Dq9Bk"
   },
   "source": [
    "<Tip>\n",
    "\n",
    "You will see a warning about some of the pretrained weights not being used and some weights being randomly\n",
    "initialized. Don't worry, this is completely normal! The pretrained head of the BERT model is discarded, and replaced with a randomly initialized classification head. You will fine-tune this new model head on your sequence classification task, transferring the knowledge of the pretrained model to it.\n",
    "\n",
    "</Tip>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjOpyGX_q9Bk"
   },
   "source": [
    "### Training hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PkAPBm3Hq9Bk"
   },
   "source": [
    "Next, create a [TrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments) class which contains all the hyperparameters you can tune as well as flags for activating different training options. For this tutorial you can start with the default training [hyperparameters](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments), but feel free to experiment with these to find your optimal settings.\n",
    "\n",
    "Specify where to save the checkpoints from your training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "6CmxM1Jaq9Bl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dkss7-poq9Bl"
   },
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bnUXCQpmq9Bn"
   },
   "source": [
    "[Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) does not automatically evaluate model performance during training. You will need to pass [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) a function to compute and report metrics. The ðŸ¤— Datasets library provides a simple [`accuracy`](https://huggingface.co/metrics/accuracy) function you can load with the `load_metric` (see this [tutorial](https://huggingface.co/docs/datasets/metrics.html) for more information) function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "id": "cnkwdUisq9Bo"
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.2643249034881592,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Downloading builder script",
       "rate": null,
       "total": 1652,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5316442aed694d40b8b41ae3d45cbb2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.65k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R70GPBpqq9Bp"
   },
   "source": [
    "Call `compute` on `metric` to calculate the accuracy of your predictions. Before passing your predictions to `compute`, you need to convert the predictions to logits (remember all ðŸ¤— Transformers models return logits):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "id": "MNXHIrXWq9Bp"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8JSs6yTq9Bp"
   },
   "source": [
    "If you'd like to monitor your evaluation metrics during fine-tuning, specify the `evaluation_strategy` parameter in your training arguments to report the evaluation metric at the end of each epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "rUSfYcUuq9Bq"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-kmvyaLsq9Bq"
   },
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uB2qhAwnq9Bq"
   },
   "source": [
    "Create a [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) object with your model, training arguments, training and test datasets, and evaluation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "L590Jm-_q9Bq"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bu5qHEeq9Br"
   },
   "source": [
    "Then fine-tune your model by calling [train()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "XOxAoWSgq9Br"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 108\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [42/42 35:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.332739</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.388298</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.171042</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 13\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 13\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 13\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training done\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "print(\"training done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "byDHfEtVq9Br"
   },
   "source": [
    "<a id='pytorch_native'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEkZ3UiBq9Bt"
   },
   "source": [
    "Load your model with the number of expected labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in zf_model\\config.json\n",
      "Model weights saved in zf_model\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"zf_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UYKxPQi2q9Bt"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
